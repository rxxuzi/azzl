#!/usr/bin/env python3
import requests
import pdfplumber
import os
import sys
import datetime
import argparse
from markdownify import markdownify as md

LOG_FILE = ".pdf2db.log"
DATABASE_DIR = "./database"
DATABASE_FILE = os.path.join(DATABASE_DIR, "pdf.db")
PDF_SAVE_DIR = os.path.join(DATABASE_DIR, "pdf")  # PDF ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª


def already_crawled(url: str, log_file: str) -> bool:
    """
    ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æŒ‡å®šã—ãŸURLãŒæ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ç¢ºèªã™ã‚‹ã€‚
    """
    if not os.path.exists(log_file):
        return False
    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            if url in line:
                return True
    return False


def download_pdf(url):
    """
    æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {url}")
    response = requests.get(url)
    response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯

    pdf_filename = url.split("/")[-1]  # URL ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
    temp_pdf_path = os.path.join(DATABASE_DIR, "temp.pdf")

    with open(temp_pdf_path, "wb") as f:
        f.write(response.content)

    return temp_pdf_path, pdf_filename


def extract_text_from_pdf(pdf_path):
    """
    PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚
    """
    print("ğŸ” PDF ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºä¸­...")
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            extracted_text = page.extract_text()
            if extracted_text:
                text += extracted_text + "\n"
    return text.strip()


def save_to_database(text):
    """
    æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã«1è¡Œãšã¤ä¿å­˜ã™ã‚‹ã€‚
    """
    os.makedirs(DATABASE_DIR, exist_ok=True)
    with open(DATABASE_FILE, "a", encoding="utf-8") as db_file:
        for line in text.splitlines():
            db_file.write(line.strip() + "\n")


def log_url(url, pdf_filename):
    """
    å‡¦ç†ã—ãŸURLã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã™ã‚‹ã€‚
    """
    with open(LOG_FILE, "a", encoding="utf-8") as log_file:
        timestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
        log_file.write(f"[{timestamp}] {url} {pdf_filename}\n")


def save_pdf(pdf_path, pdf_filename):
    """
    PDFã‚’ä¿å­˜ã™ã‚‹ï¼ˆ-s ã‚ªãƒ—ã‚·ãƒ§ãƒ³æœ‰åŠ¹æ™‚ï¼‰
    """
    os.makedirs(PDF_SAVE_DIR, exist_ok=True)
    save_path = os.path.join(PDF_SAVE_DIR, pdf_filename)
    os.rename(pdf_path, save_path)
    print(f"ğŸ“‚ PDF ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
    return save_path


def fetch_and_store(url: str, save_pdf_flag: bool = False, force_update: bool = False):
    """
    URL ã‹ã‚‰ PDF ã‚’å–å¾—ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ Markdown å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®URLãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ-ræ™‚ã¯å¼·åˆ¶å®Ÿè¡Œï¼‰
    if not force_update and already_crawled(url, LOG_FILE):
        print(f"âœ… {url} ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™ã€‚")
        return

    # PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    try:
        pdf_path, pdf_filename = download_pdf(url)
    except Exception as e:
        print(f"âŒ PDFã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    text = extract_text_from_pdf(pdf_path)
    if not text:
        print("âš ï¸ æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ Markdown ã«å¤‰æ›
    markdown_text = md(text)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    save_to_database(markdown_text)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {DATABASE_FILE}")

    # ãƒ­ã‚°ã«è¨˜éŒ²
    log_url(url, pdf_filename)
    print(f"ğŸ“ ãƒ­ã‚°è¨˜éŒ²å®Œäº†: {LOG_FILE}")

    # PDF ã‚’ä¿å­˜ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæœ‰åŠ¹ãªã‚‰ä¿å­˜
    if save_pdf_flag:
        save_pdf(pdf_path, pdf_filename)
    else:
        os.remove(pdf_path)  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤


def reprocess_logged_urls(save_pdf_flag: bool = False):
    """
    .pdf2db.log ã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹URLã‚’ã™ã¹ã¦å†å–å¾—ã™ã‚‹ã€‚
    """
    if not os.path.exists(LOG_FILE):
        print("âŒ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        sys.exit(1)

    print("ğŸ”„ éå»ã«å–å¾—ã—ãŸPDFã‚’å†å‡¦ç†ä¸­...")
    with open(LOG_FILE, "r", encoding="utf-8") as log_f:
        lines = log_f.readlines()

    urls = [line.split(" ")[1] for line in lines if len(line.split(" ")) > 1]

    for url in urls:
        fetch_and_store(url, save_pdf_flag, force_update=True)


def main():
    parser = argparse.ArgumentParser(
        description="PDF ã‹ã‚‰ Markdown ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "url",
        nargs="?",
        help="å…¥åŠ›ã¨ãªã‚‹ PDF ã® URLï¼ˆã¾ãŸã¯ -r ã§éå»ã® URL ã‚’å†å‡¦ç†ï¼‰"
    )
    parser.add_argument(
        "-r", "--reprocess",
        action="store_true",
        help="éå»ã® PDF URL ã‚’å†å–å¾—"
    )
    parser.add_argument(
        "-s", "--save",
        action="store_true",
        help="PDF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"
    )

    args = parser.parse_args()

    if args.reprocess:
        reprocess_logged_urls(args.save)
    elif args.url:
        fetch_and_store(args.url, args.save)
    else:
        print("Usage: python p2d.py <PDF_URL> | -r [-s]")
        sys.exit(1)


if __name__ == "__main__":
    main()
