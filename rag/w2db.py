#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import re
import sys
import os
import datetime

LOG_FILE = ".wiki2db.log"
DATABASE_DIR = "./database"
DATABASE_FILE = os.path.join(DATABASE_DIR, "wiki.db")


def already_crawled(url: str, log_file: str) -> bool:
    """
    ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æŒ‡å®šã—ãŸURLãŒæ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ç¢ºèªã™ã‚‹ã€‚
    """
    if not os.path.exists(log_file):
        return False
    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            if url in line:
                return True
    return False


def fetch_and_store(url: str, force_update: bool = False):
    """
    URL ã‹ã‚‰ HTML ã‚’å–å¾—ã—ã€ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤ã—ãŸå¾Œã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®URLãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ-ræ™‚ã¯å¼·åˆ¶å®Ÿè¡Œï¼‰
    if not force_update and already_crawled(url, LOG_FILE):
        print(f"âœ… {url} ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™ã€‚")
        return

    print(f"ðŸŒ URLå–å¾—ä¸­: {url}")

    # URLã‹ã‚‰HTMLã‚’å–å¾—
    try:
        response = requests.get(url)
        response.raise_for_status()
    except Exception as e:
        print(f"âŒ URLã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # BeautifulSoupã§HTMLã‚’ãƒ‘ãƒ¼ã‚¹
    soup = BeautifulSoup(response.text, "html.parser")
    content_div = soup.find(id="content")

    if content_div is None:
        print("âŒ ID 'content' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
    for elem in content_div.find_all(attrs={"role": "presentation"}):
        elem.decompose()
    for elem in content_div.find_all(class_=["vector-page-toolbar", "vector-toc-landmark", "vector-column-end"]):
        elem.decompose()
    for elem in content_div.find_all(id="p-lang-btn"):
        elem.decompose()

    # H1ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
    title_tag = soup.find("h1")
    page_title = title_tag.get_text(strip=True) if title_tag else "No Title"

    # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€ä½™åˆ†ãªç©ºç™½ã‚’ã¾ã¨ã‚ã¦1è¡Œã«æ•´å½¢
    raw_text = content_div.get_text(separator=" ", strip=True)
    one_line_text = re.sub(r'\s+', ' ', raw_text)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆï¼ˆå­˜åœ¨ã—ãªã‘ã‚Œã°ï¼‰
    os.makedirs(DATABASE_DIR, exist_ok=True)

    # wiki.dbã«è¿½è¨˜
    try:
        with open(DATABASE_FILE, "a", encoding="utf-8") as f:
            f.write(one_line_text + "\n")
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã§URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨˜éŒ²
    try:
        with open(LOG_FILE, "a", encoding="utf-8") as log_f:
            timestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
            log_f.write(f"[{timestamp}] {url} {page_title}\n")
    except Exception as e:
        print(f"âŒ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {url} ({page_title})")


def reprocess_logged_urls():
    """
    .wiki2db.log ã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹URLã‚’ã™ã¹ã¦å†å–å¾—ã™ã‚‹ã€‚
    """
    if not os.path.exists(LOG_FILE):
        print("âŒ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        sys.exit(1)

    print("ðŸ”„ éŽåŽ»ã«å–å¾—ã—ãŸURLã‚’å†å‡¦ç†ä¸­...")
    with open(LOG_FILE, "r", encoding="utf-8") as log_f:
        lines = log_f.readlines()

    urls = [line.split(" ")[1] for line in lines if len(line.split(" ")) > 1]

    for url in urls:
        fetch_and_store(url, force_update=True)


def main():
    if len(sys.argv) < 2:
        print("Usage: python w2db.py URL | -r")
        sys.exit(1)

    if sys.argv[1] == "-r":
        reprocess_logged_urls()
    else:
        fetch_and_store(sys.argv[1])


if __name__ == "__main__":
    main()
