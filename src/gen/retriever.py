# gen/retriever.py

import logging
import os
import sys
from typing import List, Dict, Any

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from gen.database import load_vector_db
from gen.search import generate_embedding, search_vector_db
from config import TOP_N, THRESHOLD

logger = logging.getLogger(__name__)

OLLAMA_ENDPOINT = os.getenv("OLLAMA_ENDPOINT", "http://127.0.0.1:11434")
CROSS_ENCODER_MODEL = os.getenv("CROSS_ENCODER_MODEL", "cross-encoder/ms-marco-MiniLM-L-6-v2")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
_tokenizer = None
_model = None


def init_retriever() -> None:
    """
    ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘å‘¼ã³å‡ºã—ã€
    Cross Encoder ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    """
    global _tokenizer, _model
    if _tokenizer is None or _model is None:
        logger.info("Initializing Cross Encoder model...")
        try:
            _tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL)
            _model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL)
            _model.eval()
            logger.info(f"Cross Encoder model loaded successfully: {CROSS_ENCODER_MODEL}")
        except Exception as e:
            logger.error(f"ğŸ˜£ Failed to load Cross Encoder model: {e}")
            sys.exit(1)
    else:
        logger.info("Cross Encoder model is already initialized.")


def rerank_candidates(query: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Cross Encoder ã«ã‚ˆã‚‹å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡Œã†ã€‚
    candidates ã¯ã€å„å€™è£œãŒ "document" ã‚­ãƒ¼ã‚’æŒã¤è¾æ›¸ã®ãƒªã‚¹ãƒˆã¨ã™ã‚‹ã€‚
    """
    global _tokenizer, _model
    if _model is None or _tokenizer is None:
        # ãƒ¢ãƒ‡ãƒ«æœªåˆæœŸåŒ–ã®å ´åˆã¯ã€ãã®ã¾ã¾è¿”ã™ã‹ã€ä¾‹å¤–ã‚’æŠ•ã’ã‚‹
        logger.warning("Cross Encoder model is not initialized. Skipping rerank.")
        return candidates

    for candidate in candidates:
        pair = (query, candidate["document"])
        inputs = _tokenizer(
            pair[0], pair[1],
            return_tensors="pt",
            truncation=True,
            padding=True
        )
        with torch.no_grad():
            outputs = _model(**inputs)
            # cross-encoder/ms-marco-MiniLM-L-6-v2 ã¯ [batch_size, 1] çš„ãªå‡ºåŠ›ã«ãªã‚‹ã€‚
            # logits.item() ã§ã‚¹ã‚³ã‚¢ã‚’ã‚¹ã‚«ãƒ©ãƒ¼ã¨ã—ã¦å–å¾—
            score = outputs.logits.item()
        candidate["rerank_score"] = score

    # rerank_score ã«åŸºã¥ã„ã¦é™é †ã‚½ãƒ¼ãƒˆ
    return sorted(candidates, key=lambda x: x.get("rerank_score", 0.0), reverse=True)


def retrieve_context(question: str, top_n: int = TOP_N, threshold: float = THRESHOLD) -> str:
    """
    è³ªå•ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã€Dense æ¤œç´¢ã¨ Cross Encoder ã«ã‚ˆã‚‹å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§
    ä¸Šä½ N ä»¶ã®é–¢é€£æ–‡æ›¸ã‚’å–å¾—ã™ã‚‹ã€‚

    Args:
        question (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•æ–‡
        top_n (int): ä¸Šä½ä½•ä»¶ã‚’å–ã‚Šå‡ºã™ã‹ (config.py ã§ç®¡ç†)
        threshold (float): ã‚¹ã‚³ã‚¢ã—ãã„å€¤

    Returns:
        str: å–å¾—ã—ãŸæ–‡æ›¸ã‚’æ”¹è¡ŒåŒºåˆ‡ã‚Šã§ã¾ã¨ã‚ãŸæ–‡å­—åˆ—
    """
    try:
        vector_db = load_vector_db()
        if not vector_db:
            logger.warning("Vector DB is empty.")
            return ""

        # Dense Retrieval
        query_embedding = generate_embedding(question)
        dense_candidates = search_vector_db(query_embedding, vector_db)
        logger.info(f"Dense Retrieval: {len(dense_candidates)} candidates obtained.")

        # Cross Encoder ã«ã‚ˆã‚‹å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        reranked_candidates = rerank_candidates(question, dense_candidates)
        logger.info("Reranking completed.")

        # ä¸Šä½ top_n ä»¶ã‚’æ¡ç”¨ (threshold ã§é™¤å¤–ã™ã‚‹ãªã‚‰ã“ã“ã§åˆ¤å®š)
        final_candidates = reranked_candidates[:top_n]

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã‚’ã¾ã¨ã‚ã¦è¿”ã™
        context = "\n".join([f"ãƒ»{doc['document']}" for doc in final_candidates])
        return context

    except Exception as e:
        logger.error(f"Error in retrieve_context: {e}")
        return ""
