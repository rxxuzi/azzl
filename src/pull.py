import sys
import os
import argparse
import json
import time
import dotenv
import threading
from queue import Queue
from rich.progress import track
from rich.prompt import Confirm
from rich.console import Console
from gen.search import generate_embedding, setup_chroma_collection
from gen.database import save_vector_db

# .envã‚’èª­ã¿è¾¼ã‚€
dotenv.load_dotenv()

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ï¼ˆå…¥åŠ›ã¯TXTå½¢å¼ã«å¤‰æ›´ï¼‰
DEFAULT_DATABASE = os.getenv("DATABASE", "../db/database.txt")
DEFAULT_VECTOR_DB = os.getenv("VECTOR_DB", "../db/vec.db")
DEFAULT_THREADS = 4  # ä¸¦åˆ—å‡¦ç†ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ãƒ¬ãƒƒãƒ‰æ•°

console = Console()

def load_documents_from_files(input_paths: list) -> list:
    """
    è¤‡æ•°ã®TXTãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ã€‚
    å„è¡ŒãŒ1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãªã‚Šã€ç©ºè¡Œã¯ç„¡è¦–ã—ã¾ã™ã€‚
    """
    documents = []
    for path in input_paths:
        if not os.path.exists(path):
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ« '{path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            sys.exit(1)
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    documents.append(line)
    return documents

def worker(queue: Queue, results: list):
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ï¼šã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã—ã€åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"""
    while not queue.empty():
        i, doc = queue.get()
        embedding = generate_embedding(doc)
        results[i] = {"id": str(i), "embedding": embedding, "document": doc}
        queue.task_done()

def create_vector_db(input_files: list, output_file: str, threads: int, force: bool = False, verbose: bool = False):
    """
    è¤‡æ•°ã®å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ä¸¦åˆ—å‡¦ç†ã§åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã€
    ãƒ™ã‚¯ãƒˆãƒ«DBã‚’ä½œæˆã—ã¦å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚
    """
    start_time = time.time()

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    if not force and os.path.exists(output_file):
        print(f"âš ï¸  è­¦å‘Š: å‡ºåŠ›å…ˆ '{output_file}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")
        if not Confirm.ask("ä¸Šæ›¸ãã—ã¾ã™ã‹ï¼Ÿ"):
            print("ğŸ›‘ å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
            sys.exit(1)

    print("ğŸ”§ ChromaDB ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
    collection = setup_chroma_collection()

    # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    print(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {', '.join(input_files)}")
    documents = load_documents_from_files(input_files)

    # ã‚­ãƒ¥ãƒ¼ã¨çµæœãƒªã‚¹ãƒˆã®ä½œæˆ
    queue = Queue()
    results = [None] * len(documents)

    for i, doc in enumerate(documents):
        queue.put((i, doc))

    print(f"âš¡ ä¸¦åˆ—å‡¦ç† ({threads}ã‚¹ãƒ¬ãƒƒãƒ‰) ã§åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆä¸­...")

    # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½œæˆ
    workers = []
    for _ in range(threads):
        thread = threading.Thread(target=worker, args=(queue, results))
        thread.start()
        workers.append(thread)

    # é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤ºã—ãªãŒã‚‰å¾…æ©Ÿ
    for _ in track(range(len(documents)), description="ğŸ“ ãƒ™ã‚¯ãƒˆãƒ«DBã‚’ä½œæˆä¸­..."):
        queue.join()

    # å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…ã¤
    for thread in workers:
        thread.join()

    # ChromaDB ã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    for item in results:
        collection.add(
            ids=[item["id"]],
            embeddings=[item["embedding"]],
            documents=[item["document"]]
        )

    save_vector_db_to_file(results, output_file)

    end_time = time.time()
    elapsed_time = end_time - start_time

    print(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«DBä½œæˆå®Œäº†: {output_file}")
    if verbose:
        db_size = os.path.getsize(output_file) / (1024 * 1024)  # MBå˜ä½
        print(f"ğŸ•’ å‡¦ç†æ™‚é–“: {elapsed_time:.2f} ç§’")
        print(f"ğŸ“¦ DB ã‚µã‚¤ã‚º: {db_size:.2f} MB")
        print(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")

def save_vector_db_to_file(vector_db: list, output_path: str):
    """ãƒ™ã‚¯ãƒˆãƒ«DBã‚’JSONå½¢å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚"""
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(vector_db, f, ensure_ascii=False, indent=4)

def main():
    parser = argparse.ArgumentParser(
        description="ãƒ™ã‚¯ãƒˆãƒ«DBä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ (RAGç”¨)",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "input_files",
        nargs="*",
        help="å…¥åŠ›ã¨ãªã‚‹ TXT ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ (çœç•¥æ™‚ã¯ .env ã® DATABASE ã‚’ä½¿ç”¨)"
    )
    parser.add_argument(
        "-o", "--output",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (çœç•¥æ™‚ã¯ .env ã® VECTOR_DB ã‚’ä½¿ç”¨)"
    )
    parser.add_argument(
        "-t", "--threads",
        type=int,
        default=DEFAULT_THREADS,
        help=f"ä¸¦åˆ—å‡¦ç†ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_THREADS})"
    )
    parser.add_argument(
        "-f", "--force",
        action="store_true",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã€ä¸Šæ›¸ãã—ã¾ã™ã€‚"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™ (å‡¦ç†æ™‚é–“ã€DBã‚µã‚¤ã‚ºãªã©)ã€‚"
    )

    args = parser.parse_args()

    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œãªã‹ã£ãŸå ´åˆã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
    input_files = args.input_files if args.input_files else [DEFAULT_DATABASE]
    output_path = args.output if args.output else DEFAULT_VECTOR_DB

    print(f"ğŸ“Œ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(input_files)}")
    print(f"ğŸ“Œ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
    print(f"ğŸ”„ ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {args.threads}")

    create_vector_db(input_files, output_path, args.threads, force=args.force, verbose=args.verbose)

if __name__ == "__main__":
    main()
